/* See COPYRIGHT for copyright information. */

#include "../inc/x86.h"
#include "../inc/mmu.h"
#include "../inc/types.h"
#include "../inc/error.h"
#include "../inc/string.h"
#include "../inc/assert.h"
#include "../inc/memlayout.h"

#include "pmap.h"
#include "kclock.h"
#include "env.h"
#include "buddydef.h"
#include "cpu.h"

/* These variables are set by i386_detect_memory() */
size_t npages; /* Amount of physical memory (in pages) */
static size_t npages_basemem; /* Amount of base memory (in pages) */

/* These variables are set in mem_init() */
pde_t *kern_pgdir;                       /* Kernel's initial page directory */
struct page_info *pages;                 /* Physical page state array */
static struct page_info *page_free_list; /* Free list of physical pages */

/***************************************************************
 * Detect machine's physical memory setup.
 ***************************************************************/

static int nvram_read(int r) {
    return mc146818_read(r) | (mc146818_read(r + 1) << 8);
}

static void i386_detect_memory(void) {
    size_t npages_extmem;

    /* Use CMOS calls to measure available base & extended memory.
     * (CMOS calls return results in kilobytes.) */
    npages_basemem = (nvram_read(NVRAM_BASELO) * 1024) / PGSIZE;
    npages_extmem = (nvram_read(NVRAM_EXTLO) * 1024) / PGSIZE;

    /* Calculate the number of physical pages available in both base and
     * extended memory. */
    if (npages_extmem)
        npages = (EXTPHYSMEM / PGSIZE) + npages_extmem;
    else
        npages = npages_basemem;

    cprintf("Physical memory: %uK available, base = %uK, extended = %uK\n",
            npages * PGSIZE / 1024,
            npages_basemem * PGSIZE / 1024,
            npages_extmem * PGSIZE / 1024);
}


/***************************************************************
 * Set up memory mappings above UTOP.
 ***************************************************************/

static void mem_init_mp(void);
static void boot_map_region(pde_t *pgdir, uintptr_t va, size_t size,
        physaddr_t pa, int perm);
static void check_page_free_list(bool only_low_memory);

static void check_page_alloc(void);
static void check_kern_pgdir(void);
static physaddr_t check_va2pa(pde_t *pgdir, uintptr_t va);
static void check_page(void);
static void check_page_installed_pgdir(void);
static void check_page_hugepages(void);

/* This simple physical memory allocator is used only while JOS is setting up
 * its virtual memory system.  page_alloc() is the real allocator.
 *
 * If n>0, allocates enough pages of contiguous physical memory to hold 'n'
 * bytes.  Doesn't initialize the memory.  Returns a kernel virtual address.
 *
 * If n==0, returns the address of the next free page without allocating
 * anything.
 *
 * If we're out of memory, boot_alloc should panic.
 * This function may ONLY be used during initialization, before the
 * page_free_list list has been set up. */
static void *boot_alloc(uint32_t n) {
    static char *nextfree = 0; /* virtual address of next byte of free memory */
    char *result;

    /* Initialize nextfree if this is the first time. 'end' is a magic symbol
     * automatically generated by the linker, which points to the end of the
     * kernel's bss segment: the first virtual address that the linker did *not*
     * assign to any kernel code or global variables. */
    if (!nextfree) {
        extern char end[];
        nextfree = ROUNDUP((char *) end, PGSIZE);
    }

    /* Allocate a chunk large enough to hold 'n' bytes, then update nextfree.
     * Make sure nextfree is kept aligned to a multiple of PGSIZE.
     *
     * LAB 1: Your code here.
     */
    //return pointer to freemem if n=0
    if (n == 0)
        return (void *) nextfree;

    //Check if enough free memory exists
    // If we reached true OOM state, PANIC!
    uint32_t usage_cp, max;
    usage_cp = ((uint32_t) nextfree - KERNBASE) + n;
    max = 4 << 20; //4MiB
    dprintf("Kernel boot alloc:\n\tnew alloc: %uK\n\tcurrent Usage %uK\n\tmax Usage: %uK\n", n / 1024, usage_cp / 1024,
            max / 1024);
    if (usage_cp >= max)
        panic("Out of Memory PANIC: boot allocation failed.");

    //nextfree points to free memory, keep this value for return
    void *newAlloc = (void *) nextfree;
    nextfree += n; //increment next free by n
    nextfree = ROUNDUP((char *) nextfree, PGSIZE);

    /*
     * Before we check our max size, let us first discuss what this should be:
     * Currently we are in virtual memory and the variable end[] is places just after the kernel.
     * This means that our *end is somewhere here:
     * ---------------------------------------------------------------
     * | [vkernel_legacy]  [free]            [vkernel] [end]   |     | end of virtual memory (uint32 max)
     * ---------------------------------------------------------------
     *                                               true end  ^
     * NOTE: This is the end of virtual memory!!!, 261MB is not what we have! we have less! (66,5mb)
     *
     * reference:
     * #Bootstrap GDT
        .p2align 2                                # force 4 byte alignment
        gdt:
          SEG_NULL              # null seg
          SEG(STA_X|STA_R, 0x0, 0xffffffff) # code seg
          SEG(STA_W, 0x0, 0xffffffff)           # data seg
     *
     * Thanks Japan!
     * http://pekopeko11.sakura.ne.jp/unix_v6/xv6-book/en/_images/F2-2.png
     */

    return newAlloc;
}

/*
 * Bootstrapping flag
 *  Only 4M is mapped when this flag is set
 */
static uint8_t boot_low_mem=1;


/*
 * Set up a two-level page table:
 *    kern_pgdir is its linear (virtual) address of the rnpages_basememoot
 *
 * This function only sets up the kernel part of the address space (ie.
 * addresses >= UTOP).  The user part of the address space will be setup later.
 *
 * From UTOP to ULIM, the user is allowed to read but not write.
 * Above ULIM the user cannot read or write.
 */
void mem_init(void) {
    uint32_t cr0;
    size_t n;

    /* Find out how much memory the machine has (npages & npages_basemem). */
    i386_detect_memory();

    /*********************************************************************
     * create initial page directory.
     */
    kern_pgdir = (pde_t *) boot_alloc(PGSIZE);
    memset(kern_pgdir, 0, PGSIZE);

    /*********************************************************************
     * Recursively insert PD in itself as a page table, to form a virtual page
     * table at virtual address UVPT.
     * (For now, you don't have understand the greater purpose of the following
     * line.)
     */

    /* Permissions: kernel R, user R */
    kern_pgdir[PDX(UVPT)] = PADDR(kern_pgdir) | PTE_U | PTE_P;

    /*********************************************************************
     * Allocate an array of npages 'struct page_info's and store it in 'pages'.
     * The kernel uses this array to keep track of physical pages: for each
     * physical page, there is a corresponding struct page_info in this array.
     * 'npages' is the number of physical pages in memory.  Your code goes here.
     */
    //npages of boot_alloc required for paging
    dprintf("Allocating %u pages.\n", npages);
    pages = boot_alloc(sizeof (struct page_info) * npages); //This panics if Out of Memory


     /*********************************************************************
     * Make 'envs' point to an array of size 'NENV' of 'struct env'.
     * LAB 3: Your code here.
     */
    dprintf("Allocating %u user environments.\n", NENV);
    envs = boot_alloc(sizeof(struct env) * NENV);

    /*********************************************************************
     * Now that we've allocated the initial kernel data structures, we set
     * up the list of free physical pages. Once we've done so, all further
     * memory management will go through the page_* functions. In particular, we
     * can now map memory using boot_map_region or page_insert.
     */
    page_init();

    check_page_free_list((bool)1);
    check_page_alloc();
    check_page();

    /*********************************************************************
     * Now we set up virtual memory */

    /*********************************************************************
     * Map 'pages' read-only by the user at linear address UPAGES
     * Permissions:
     *    - the new image at UPAGES -- kernel R, user R
     *      (ie. perm = PTE_U | PTE_P)
     *    - pages itself -- kernel RW, user NONE
     * Your code goes here:
     */
    boot_map_region(kern_pgdir, UPAGES, PTSIZE, ((uint32_t)pages) - KERNBASE, PTE_BIT_USER | PTE_BIT_PRESENT);
    boot_map_region(kern_pgdir, ((uint32_t)pages), PTSIZE, ((uint32_t)pages) - KERNBASE, PTE_BIT_RW | PTE_BIT_PRESENT);

    /* TOM: this block was addded by upstream/lab3, but seems identical to above */
    /* This is set up already by the identity mapping below. */
    /*
    boot_map_region(kern_pgdir, (uintptr_t)pages,
            ROUNDUP(sizeof(struct page_info) * npages), PADDR(pages), PTE_W);
    */
//    boot_map_region(kern_pgdir, UPAGES,
//            ROUNDUP(sizeof(struct page_info) * npages, PGSIZE),
//            PADDR(pages), PTE_U);

    /*********************************************************************
     * Map the 'envs' array read-only by the user at linear address UENVS
     * (ie. perm = PTE_U | PTE_P).
     * Permissions:
     *    - the new image at UENVS  -- kernel R, user R
     *    - envs itself -- kernel RW, user NONE
     * LAB 3: Your code here.
     */

    boot_map_region(kern_pgdir, (uintptr_t)envs, PTSIZE, PADDR(envs), PTE_BIT_RW | PTE_BIT_PRESENT);
    boot_map_region(kern_pgdir, UENVS, PTSIZE, PADDR(envs), PTE_BIT_USER | PTE_BIT_PRESENT);

    /*********************************************************************
     * Use the physical memory that 'bootstack' refers to as the kernel
     * stack.  The kernel stack grows down from virtual address KSTACKTOP.
     * We consider the entire range from [KSTACKTOP-PTSIZE, KSTACKTOP)
     * to be the kernel stack, but break this into two pieces:
     *     * [KSTACKTOP-KSTKSIZE, KSTACKTOP) -- backed by physical memory
     *     * [KSTACKTOP-PTSIZE, KSTACKTOP-KSTKSIZE) -- not backed; so if
     *       the kernel overflows its stack, it will fault rather than
     *       overwrite memory.  Known as a "guard page".
     *     Permissions: kernel RW, user NONE
     * Your code goes here:
     */

    /* Note: Dont map anything between KSTACKTOP - PTSIZE and KSTACKTOP - KTSIZE
     * leaving this as guard region.
     */

    /*********************************************************************
     * Map all of physical memory at KERNBASE.
     * Ie.  the VA range [KERNBASE, 2^32) should map to
     *      the PA range [0, 2^32 - KERNBASE)
     * We might not have 2^32 - KERNBASE bytes of physical memory, but
     * we just set up the mapping anyway.
     * Permissions: kernel RW, user NONE
     * Your code goes here:
     */
    boot_map_region(kern_pgdir, KERNBASE, 0xFFFFF000-KERNBASE, 0, PTE_BIT_RW | PTE_BIT_PRESENT);

    /* Enable Page Size Extensions for huge page support */
    lcr4(rcr4() | CR4_PSE);

    /* Initialize the SMP-related parts of the memory map. */
    mem_init_mp();

    /* Check that the initial page directory has been set up correctly. */
    check_kern_pgdir();

    /* Switch from the minimal entry page directory to the full kern_pgdir
    / * page table we just created.  Our instruction pointer should be
     * somewhere between KERNBASE and KERNBASE+4MB right now, which is
     * mapped the same way by both page tables.
     *
     * If the machine reboots at this point, you've probably set up your
     * kern_pgdir wrong. */
    lcr3(PADDR(kern_pgdir));

    check_page_free_list(0);

    /* entry.S set the really important flags in cr0 (including enabling
     * paging).  Here we configure the rest of the flags that we care about. */
    cr0 = rcr0();
    cr0 |= CR0_PE | CR0_PG | CR0_AM | CR0_WP | CR0_NE | CR0_MP;
    cr0 &= ~(CR0_TS | CR0_EM);
    lcr0(cr0);

    /* Some more checks, only possible after kern_pgdir is installed. */
    check_page_installed_pgdir();

    /* Check for huge page support */
    check_page_hugepages();
}

/*
 * Modify mappings in kern_pgdir to support SMP
 *   - Map the per-CPU stacks in the region [KSTACKTOP-PTSIZE, KSTACKTOP)
 */
static void mem_init_mp(void)
{
    /*
     * Map per-CPU stacks starting at KSTACKTOP, for up to 'NCPU' CPUs.
     *
     * For CPU i, use the physical memory that 'percpu_kstacks[i]' refers
     * to as its kernel stack. CPU i's kernel stack grows down from virtual
     * address kstacktop_i = KSTACKTOP - i * (KSTKSIZE + KSTKGAP), and is
     * divided into two pieces, just like the single stack you set up in
     * mem_init:
     *     * [kstacktop_i - KSTKSIZE, kstacktop_i)
     *          -- backed by physical memory
     *     * [kstacktop_i - (KSTKSIZE + KSTKGAP), kstacktop_i - KSTKSIZE)
     *          -- not backed; so if the kernel overflows its stack,
     *             it will fault rather than overwrite another CPU's stack.
     *             Known as a "guard page".
     *     Permissions: kernel RW, user NONE
     *
     * LAB 6: Your code here:
     */
    int i;
    uint32_t kstacktop_i;

    for(i = 0; i < NCPU; i++) {
        kstacktop_i = KSTACKTOP - i * (KSTKSIZE + KSTKGAP);

        boot_map_region(kern_pgdir, kstacktop_i-KSTKSIZE, KSTKSIZE, (uint32_t)percpu_kstacks[i], PTE_BIT_RW | PTE_BIT_PRESENT);

        /* Invalid phys addr to trigger fault when accessed */
        boot_map_region(kern_pgdir, kstacktop_i-PTSIZE, KSTKGAP, 0xFFFFF000 - KSTKGAP,0);
    }

}

/***************************************************************
 * Tracking of physical pages.
 * The 'pages' array has one 'struct page_info' entry per physical page.
 * Pages are reference counted, and free pages are kept on a linked list.
 ***************************************************************/

/*
 * Initialize page structure and memory free list.
 * After this is done, NEVER use boot_alloc again.  ONLY use the page
 * allocator functions below to allocate and deallocate physical
 * memory via the page_free_list.
 */
void page_init(void) {
    /*
     * The example code here marks all physical pages as free.
     * However this is not truly the case.  What memory is free?
     *  1) Mark physical page 0 as in use.
     *     This way we preserve the real-mode IDT and BIOS structures in case we
     *     ever need them.  (Currently we don't, but...)
     *  2) The rest of base memory, [PGSIZE, npages_basemem * PGSIZE) is free.
     *  3) Then comes the IO hole [IOPHYSMEM, EXTPHYSMEM), which must never be
     *     allocated.
     *  4) Then extended memory [EXTPHYSMEM, ...).
     *     Some of it is in use, some is free. Where is the kernel in physical
     *     memory?  Which pages are already in use for page tables and other
     *     data structures?
     *
     * Change the code to reflect this.
     * NB: DO NOT actually touch the physical memory corresponding to free
     *     pages! */
    int32_t i;
    bool is_free;
    physaddr_t page_addr;
    char *nextfree = boot_alloc((uint32_t) 0);
    uint32_t cf = 0; //free pages counter

    register rpage_control pc0;
    pc0.RPC = 0;

    for (i = npages-1; i>=0; i--) {
        page_addr = page2pa(&pages[i]);

        //List states of page
        pc0.reg.kernelPage =
                page_addr >= EXTPHYSMEM && page_addr < (uint32_t) nextfree - KERNBASE; //Kernel allocated space
        pc0.reg.IOhole = (page_addr >= IOPHYSMEM && page_addr < EXTPHYSMEM); //IO hole
        pc0.reg.bios = !i;
        //Every 1024 pages are 4mb alligned
        pc0.reg.alligned4mb = (i % HUGE_PAGE_AMOUNT) == 0;

        //debug print states
        //                cprintf("Page %u: K %u, IO %u, bios %u, 4mb alligned %u\n", i, pc0.reg.kernelPage, pc0.reg.IOhole, pc0.reg.bios, pc0.reg.alligned4mb);

        //is free if
        //          not kernel              not iohole
        is_free = !pc0.reg.kernelPage && !pc0.reg.IOhole && !pc0.reg.bios;

        cf += is_free; //just a statistic counter

        pages[i].c0.RPC = pc0.RPC;
        pages[i].pp_ref = !is_free;
        pages[i].pp_link = is_free ? page_free_list : NULL;
        pages[i].c0.reg.free = is_free;
        page_free_list = is_free ? &pages[i] : page_free_list;

    }

    dprintf("%u free pages. (%uK)\n", cf, (cf * PGSIZE) / 1024);
}

#ifdef BUDDY
static uint32_t* buddy_count;

void* buddy_merge(struct page_info* a) {
    struct page_info *b = BUDDY_GET_BUDDY_PAGE(a, a->c0.reg.buddy_order);

    //a must be free
    assert(a->pp_ref == 0);

    //Before merge, buddies must have the same order
    //We can only recurse to b, not a (eg O(B) < O(A))
    if (a->c0.reg.buddy_order > b->c0.reg.buddy_order)
        b = buddy_merge(b);

    assert(a->c0.reg.buddy_order < b->c0.reg.buddy_order); //this may not exist


    //If b has merged, merge A and B

}

void* buddy_split(struct page_info* a) {
    struct page_info *b = BUDDY_GET_BUDDY_PAGE(a, a->c0.reg.buddy_order);

    //Buddies must always have the same order
    assert(b->c0.reg.buddy_order == a->c0.reg.buddy_order);

    //buddy order must not be 0
    assert(a->c0.reg.buddy_order != 0);

    //Both pages must be free
    if (a->pp_ref != 0 || b->pp_ref != 0)
        panic("Invalid split: not all pages free!");

    //decrement buddy order
    b->c0.reg.buddy_order = a->c0.reg.buddy_order--;

    //TODO: Add b to free list
    //return b to free list as it is now no longer a buddy

    //return slave buddy
    return BUDDY_GET_SLAVE(a, b);
}

inline uint32_t buddy_isfree(struct page_info * a) {
    return !(BUDDY_GET_BUDDY_PAGE(a, a->c0.reg.buddy_order))->pp_ref;
}

void buddy_init() {
    //Allocate buddy_count
    buddy_count = boot_alloc(sizeof (uint32_t) * BUDDY);

    //set zero
    memset(buddy_count, 0, sizeof (uint32_t) * BUDDY);

    //Make buddies of all free pages

}
#endif

/* Sets page_free_list to given page's linked sibling page, and clears the page's link.
 * Writes zeroes to memory if ALLOC_ZERO is passed in the flags. */
void prepare_page(struct page_info *page, int alloc_flags) {
    //assert page is free (must)
    assert(page->c0.reg.free);
    //No outstanding references
    assert(!page->pp_ref);

    page_free_list = page->pp_link;
    page->pp_ref = 0;
    page->pp_link = NULL;
    page->c0.reg.free = 0;

    if (alloc_flags & ALLOC_ZERO) {
        memset(page2kva(page), 0, PGSIZE);
    }
}

/* When ran, panics if page_free_list contains a cycle, using Floyd's
 * "slow and fast" cycle detection algorithm. */
void floyd_cycle_detection() {
    uint32_t count = 0;
    struct page_info *slow, *fast;

    slow = fast = page_free_list;

    while (true) {
        slow = slow->pp_link; // 1 hop

        if (fast->pp_link)
            fast = fast->pp_link->pp_link; // 2 hops
        else
            return; // next node null => no loop

        if (!slow || !fast) // if either hits null..no loop
            return;

        if (slow == fast) // if the two ever meet...we must have a loop
            break;
        if (++count > npages)
            break;
    }

    panic("page_free_list has cycles!");
}

int is_page_in_free_list(struct page_info *page) {
    struct page_info *entry = page_free_list;
    for(; entry; entry = entry->pp_link) {
        if(entry == page) {
            return 1;
        }
    }
    return 0;
}

/*
 * Traverses naively over all pages to find a consecutive block of the given
 * amount of pages.
 *
 * Returns NULL on failure to do so
 */
struct page_info *alloc_consecutive_pages(uint16_t amount, int alloc_flags) {
    size_t i;
    uint16_t hits = 0;
    uint32_t start_address, end_address;
    struct page_info *page_hit = NULL;

    //Find amount consecutive pages
    for (i = 0; i < npages; i++) {
        if (hits >= amount) {
            break;
        }

        //If page is free, add to hit.
        //If there are not hits, page must be 4mb alligned (see flag)

        if (pages[i].c0.reg.free && ((pages[i].c0.reg.alligned4mb || hits) || amount<1024)) {
            hits++;
            page_hit = &pages[i];

            /* TODO: remove debug assert, this is slow */
//            assert(is_page_in_free_list(&pages[i]));
        } else {
            hits = 0;
        }
    }

    if (!hits || hits < amount) {
        return NULL;
    }

    if (!page_hit) {
        panic("No page found, but hitcount is correct");
    }

    end_address = (uint32_t) page2pa(page_hit);
    start_address = (uint32_t) page2pa(page_hit - (amount - 1));

    uint32_t outed = 0;

    //Extract pages and set ref & link & free to 0
    struct page_info *current, **p_nextfree, *next;
    current = page_free_list;
    p_nextfree = &page_free_list;



    for(; current; current = next) {
        next = current->pp_link;

        //If its our page, extract
        if (page2pa(current) >= start_address && page2pa(current) <= end_address) {
            current->pp_link = 0;
            current->pp_link = NULL;
            current->c0.reg.free = 0;
            *p_nextfree = next;
            outed++;
        } else
            p_nextfree = &current->pp_link;


    }

    struct page_info *result = pa2page((physaddr_t) start_address);
    assert(outed == amount);
    assert(!result->pp_link);

    if (page_free_list == pa2page((physaddr_t) end_address)) {
        page_free_list = result - 1;
    }

    return result;
}


struct page_info * remove_page_from_freelist(struct page_info * pp) {
    //If the page is free, it should not be in free list

    struct page_info * i, **p, *page;
    for(i=page_free_list; i!=NULL; i=i->pp_link) {
        if (pp==i) {
            //Our page in low VM
            assert(i->c0.reg.free);
            *p = i->pp_link;

            page = i;
            //Prepare page
            page->pp_link = NULL;
            page->c0.reg.free = 0;

            return page;
        }

        p=&i->pp_link;
    }


    if (pp->c0.reg.free)
        cprintf("Warning: Page not found in remove_page while free was set!\n");

    pp->c0.reg.free = 0;
    return pp; //Page not found
}

/*
 * Allocates a physical page.
 * If (alloc_flags & ALLOC_ZERO), fills the entire
 * returned physical page with '\0' bytes.  Does NOT increment the reference
 * count of the page - the caller must do these if necessary (either explicitly
 * or via page_insert).
 * If (alloc_flags & ALLOC_PREMAPPED), returns a physical page from the
 * initial pool of mapped pages.
 *
 * Be sure to set the pp_link field of the allocated page to NULL so
 * page_free can check for double-free bugs.
 *
 * Returns NULL if out of free memory.
 *
 * Hint: use page2kva and memset
 *
 * 4MB huge pages:
 * Come back later to extend this function to support 4MB huge page allocation.
 * If (alloc_flags & ALLOC_HUGE), returns a huge physical page of 4MB size.
 */
struct page_info *page_alloc(int alloc_flags) {
    struct page_info *page = 0;

    //Check page free list
    if (!page_free_list) {
        return NULL;
    }

    if (alloc_flags & ALLOC_HUGE) {
        page = alloc_consecutive_pages((uint16_t) HUGE_PAGE_AMOUNT, alloc_flags); //CAN RETURN NULL
        if (!page)
            return page; //return null
        page->c0.reg.huge = 1;
        return page;
    }

    /* Pop the top page from the free list */
    if (!((alloc_flags & ALLOC_PREMAPPED) || boot_low_mem))
        page = page_free_list;
    else {
        struct page_info **p, * i;

        p = &page_free_list;
        for(i=page_free_list; i!=NULL; i=i->pp_link) {
            uint32_t pa = page2pa(i);

            if (pa < 4<<20) {
                //Our page in low VM
                assert(i->c0.reg.free);
                *p = i->pp_link;

                page = i;
                //Prepare page
                page->pp_ref = 0;
                page->pp_link = NULL;
                page->c0.reg.free = 0;

                if (alloc_flags & ALLOC_ZERO) {
                    memset(page2kva(page), 0, PGSIZE);
                }

                return page;
            }

            p=&i->pp_link;
        }

        return NULL; //No success in finding a page
    }

    assert(page);
    prepare_page(page, alloc_flags);

    return page;
}

/*
 * Return a page to the free list.
 * (This function should only be called when pp->pp_ref reaches 0.)
 */
void page_free(struct page_info *pp) {
    /* Fill this function in
     * Hint: You may want to panic if pp->pp_ref is nonzero or
     * pp->pp_link is not NULL. */
    uint32_t amount = 1, i, page_index;
    struct page_info *current;

    assert(pp != 0);

    if (pp->pp_ref) {
        panic("Page contained nonzero refcount during free()");
    }

    if (pp->c0.reg.huge) {
        amount = HUGE_PAGE_AMOUNT;
    }

    int warn = 0;

    for (current = pp + amount - 1; current >= pp; current--) {
        if (!current) {
            panic("Page in page_free() is undefined");
        }
        if (current->pp_link || current->c0.reg.free) {
            warn++;
            continue;
        }
        if (current != page_free_list) {
            current->pp_link = page_free_list;
            page_free_list = current;
        }

        //set flag to free
        current->c0.reg.free = 1;
    }

    if (warn)
        cprintf("Warning: %u pages in page_free() were already marked as free.\n", warn);
}

/*
 * Decrement the reference count on a page,
 * freeing it if there are no more refs.
 */
void page_decref(struct page_info *pp) {
    dprintf("page has %d refs remaining.\n", pp->pp_ref - 1);
    assert(pp->pp_ref > 0);
    if (--pp->pp_ref == 0)
        page_free(pp);
}

/*
 * Given 'pgdir', a pointer to a page directory, pgdir_walk returns
 * a pointer to the page table entry (PTE) for linear address 'va'.
 * This requires walking the two-level page table structure.
 *
 * For normal 4K paging support:
 *  The relevant page table page might not exist yet.
 *  If this is true, and create == 0, then pgdir_walk returns NULL.
 *  Otherwise, if CREATE_NORMAL flag is set in the 'create' argument,
 *  pgdir_walk allocates a new page table page with page_alloc.
 *    - If the allocation fails, pgdir_walk returns NULL.
 *    - Otherwise, the new page's reference count is incremented,
 *      the page is cleared,
 *      and pgdir_walk returns a pointer into the new page table page.
 *
 * For huge 4MB paging support:
 *  Is two-level walk required in this case?
 *  If the relevant page table entry does not exist and create == false,
 *  then, pgdir_walk returns NULL.
 *  Otherwise, if CREATE_HUGE flag is set in the 'create' argument,
 *  pgdir_walk() returns a pointer to page table entry for the huge page.

 * Hint 1: you can turn a struct page_info* into the physical address of the
 * page it refers to with page2pa() from kern/pmap.h.
 *
 * Hint 2: the x86 MMU checks permission bits in both the page directory
 * and the page table, so it's safe to leave permissions in the page
 * more permissive than strictly necessary.
 *
 * Hint 3: look at inc/mmu.h for useful macros that manipulate page
 * table and page directory entries.
 */
pte_t *pgdir_walk(pde_t *pgdir, const void *va, int create) {
    //Make a few assertions
    assert(pgdir);

    //Setup indexes (10, 10, 12)
    register uint32_t pgdi = VA_GET_PDE_INDEX(va);
    register uint32_t ptdi = VA_GET_PTE_INDEX(va);

    //Begin walking the directory and tables
    uint32_t entry = pgdir[pgdi];

    if (entry & PDE_BIT_HUGE) //if its 4M, this is your entry
            return &pgdir[pgdi];

    //determine table exist (and create if applicable)
    if (!entry) {
        //Does not exist

        //Are we told to create a page? No? NULL you go
        if (create == 0)
            return NULL;

        //Create a 4K page
        //We thus need to allocate a page for the pg table
        if (create & CREATE_NORMAL){
            struct page_info * pp = page_alloc(ALLOC_ZERO);
            if (!(pp)) //Allocate & zero out => entry
                return NULL; //Alloc failed


            pp->pp_ref++;

            entry = (uint32_t)page2pa(pp);

        }


        if (create & CREATE_HUGE) {
//            struct page_info * pp = page_alloc(ALLOC_HUGE | ALLOC_ZERO);
//            if (!pp) //Allocate & zero out => entry
//                return NULL; //Alloc failed
//            entry = (uint32_t)page2pa(pp);
            //We are a page, so we need to set the user bit
            entry |= PDE_BIT_HUGE;
        }
        else
        //entry address is no presumed valid
        //Setup flags
            entry |= PTE_BIT_RW | PTE_BIT_PRESENT | PDE_BIT_USER; // RW and Present bits set (do not set user, that is per pte entry, pde overrides those)


        //Save entry
        pgdir[pgdi] = entry;

        //flush TLB
        tlb_invalidate(pgdir, (void*)va);
    }

    //Note: We return the entry only, we dont care if the physical page exists or is present (present bit set)

    //Page table exists, determine type
    if (entry & PDE_BIT_HUGE) {
        //PDE entry to 4m page
        //Return page dir entry
        return (pte_t *) &pgdir[pgdi];
    }

    //Page dir entry links to page table
    pte_t * pgtable = (pte_t *) (PDE_GET_ADDRESS(pgdir[pgdi]));
    pgtable = (pte_t*) ((uint32_t) pgtable + KERNBASE);

    assert(pgtable);
    assert(&pgtable[ptdi]);

    return &pgtable[ptdi];
}

/*
 * Map [va, va+size) of virtual address space to physical [pa, pa+size)
 * in the page table rooted at pgdir.  Size is a multiple of PGSIZE.
 * Use permission bits perm|PTE_P for the entries.
 *
 * This function is only intended to set up the ``static'' mappings
 * above UTOP. As such, it should *not* change the pp_ref field on the
 * mapped pages.
 *
 * Hint: the TA solution uses pgdir_walk
 */
static void boot_map_region(pde_t *pgdir, uintptr_t va, size_t size, physaddr_t pa, int perm) {
    uint32_t i;
    struct page_info *page;
    for(i = 0; i < size; i += PGSIZE) {
        //Walk dir, create table if non ext., get pointer to entry, profit
        pte_t *pentry = pgdir_walk(pgdir, (void *)((uint32_t)va + i), CREATE_NORMAL);

        //Map pentry to physical region pa
        *pentry = (pa + i) | perm;
    }
    dprintf("Mapped va %#08x-%#08x to pa %#08x-%#08x\n", va, va+size, pa, pa+size);
}

/*
 * Map the physical page 'pp' at virtual address 'va'.
 * The permissions (the low 12 bits) of the page table entry
 * should be set to 'perm|PTE_P'.
 *
 * Requirements
 *   - If there is already a page mapped at 'va', it should be page_remove()d.
 *   - If necessary, on demand, a page table should be allocated and inserted
 *     into 'pgdir'.
 *   - pp->pp_ref should be incremented if the insertion succeeds.
 *   - The TLB must be invalidated if a page was formerly present at 'va'.
 *
 * Corner-case hint: Make sure to consider what happens when the same
 * pp is re-inserted at the same virtual address in the same pgdir.
 * However, try not to distinguish this case in your code, as this
 * frequently leads to subtle bugs; there's an elegant way to handle
 * everything in one code path.
 *
 * RETURNS:
 *   0 on success
 *   -E_NO_MEM, if page table couldn't be allocated
 *
 * Hint: The TA solution is implemented using pgdir_walk, page_remove,
 * and page2pa.
 *
 * Also add support for huge page insertion.
 */
int page_insert(pde_t *pgdir, struct page_info *pp, void *va, int perm) {
    register uint32_t pgdi = VA_GET_PDE_INDEX(va);
    pte_t * pentry;
    if (perm & PDE_BIT_HUGE)
        pentry = pgdir_walk(pgdir, va, CREATE_HUGE);
    else
        pentry = pgdir_walk(pgdir, va, CREATE_NORMAL);

    //pgdir_walk failure
    if (!pentry) //if entry returns null, it becomes a address...
        return -E_NO_MEM;

    //If the entry exists, remove it
    //page_remove asserts we do not delete a pg table with valid entries
    if (*pentry & PTE_BIT_PRESENT) {
        //Page exists

        //check if page is the same
        uint32_t pppaddr = page2pa(pp);

        if (PTE_GET_PHYS_ADDRESS(*pentry) != pppaddr) {
            //When pp == paddr, the page is 'simply' cleared as end result if the ref counter hits 0
            struct page_info *paddr = (struct page_info *) KADDR(PTE_GET_PHYS_ADDRESS(*pentry));
            page_remove(pgdir, va);
        }else
            pp->pp_ref--;
    }

    //fill entry
    *pentry = (uint32_t) page2pa(pp);

    //Set callers permissions
    *pentry |= perm;

    //Make sure some PGD permissions reflect their childs permissions
    uint32_t * pgde = &pgdir[pgdi];

    //overwrite permissions (and set PGD)
    //These must (for now) always be these values
    if ((*pentry) & PDE_BIT_HUGE) {
        *pentry |= (uint32_t )(PDE_BIT_HUGE | PDE_BIT_PRESENT);
    }else {
        *pentry |= PTE_BIT_PRESENT;
        *pgde |= (*pentry) & 0b11111;
    }

    //Remove page from free list, it is referenced
    pp = remove_page_from_freelist(pp);
    if (!pp) {
        *pentry = 0;
        return -E_UNSPECIFIED;
    }

    //Flush
    uint32_t addr = (uint32_t) pentry - KERNBASE;
    tlb_invalidate(pgdir, va);

    //page is now referenced
    pp->pp_ref++;

    return 0;
}

/*
 * Return the page mapped at virtual address 'va'.
 * If pte_store is not zero, then we store in it the address
 * of the pte for this page.  This is used by page_remove and
 * can be used to verify page permissions for syscall arguments,
 * but should not be used by most callers.
 *
 * Return NULL if there is no page mapped at va.
 *
 * Hint: the TA solution uses pgdir_walk and pa2page.
 */
struct page_info *page_lookup(pde_t *pgdir, void *va, pte_t **pte_store) {
    //Get entry
    pte_t * pentry = pgdir_walk(pgdir, va, 0);

    //Return null if entry pointer is null
    if (!pentry)
        return NULL;

    //Store entry
    if (pte_store)
        *pte_store = pentry;

    //If 4M, return
    if (*pentry & PDE_BIT_HUGE)
        return pa2page(PDE_GET_ADDRESS(*pentry));

    //Extract page
    uint32_t phys_addr = PTE_GET_PHYS_ADDRESS(*pentry);

    //Return page pointer if page is present (eg. not swapped out)
    if (*pentry & PTE_BIT_PRESENT)
        return pa2page(phys_addr);

    return NULL;
}

/*
 * Unmaps the physical page at virtual address 'va'.
 * If there is no physical page at that address, silently does nothing.
 *
 * Details:
 *   - The ref count on the physical page should decrement.
 *   - The physical page should be freed if the refcount reaches 0.
 *   - The pg table entry corresponding to 'va' should be set to 0.
 *     (if such a PTE exists)ME
 *   - The TLB must be invalidated if you remove an entry from
 *     the page table.
 *
 * Hint: The TA solution is implemented using page_lookup,
 *  tlb_invalidate, and page_decref.
 */
void page_remove(pde_t *pgdir, void *va) {
    //Get page and entry info
    pte_t * pentry = 0;
    struct page_info * page = page_lookup(pgdir, va, &pentry);

    //Be silent
    if (!page)
        return;

    /** Start page removal **/

    //page must not be referenced 0 times
    assert(page->pp_ref != 0);

    //decrement page
    page_decref(page);

    //reset entry
    *pentry = 0;

    //invalidate entry
    tlb_invalidate(pgdir, va);
}

/*
 * Invalidate a TLB entry, but only if the page tables being
 * edited are the ones currently in use by the processor.
 */
void tlb_invalidate(pde_t *pgdir, void *va) {
    /* Flush the entry only if we're modifying the current address space. */
    if (!curenv || curenv->env_pgdir == pgdir)
        invlpg(va);
}

/*
 * Reserve size bytes in the MMIO region and map [pa,pa+size) at this
 * location.  Return the base of the reserved region.  size does *not*
 * have to be multiple of PGSIZE.
 */
void *mmio_map_region(physaddr_t pa, size_t size)
{
    /*
     * Where to start the next region.  Initially, this is the
     * beginning of the MMIO region.  Because this is static, its
     * value will be preserved between calls to mmio_map_region
     * (just like nextfree in boot_alloc).
     */
    static uintptr_t base = MMIOBASE;

    /*
     * Reserve size bytes of virtual memory starting at base and map physical
     * pages [pa,pa+size) to virtual addresses [base,base+size).  Since this is
     * device memory and not regular DRAM, you'll have to tell the CPU that it
     * isn't safe to cache access to this memory.  Luckily, the page tables
     * provide bits for this purpose; simply create the mapping with
     * PTE_PCD|PTE_PWT (cache-disable and write-through) in addition to PTE_W.
     * (If you're interested in more details on this, see section 10.5 of IA32
     * volume 3A.)
     *
     * Be sure to round size up to a multiple of PGSIZE and to handle if this
     * reservation would overflow MMIOLIM (it's okay to simply panic if this
     * happens).
     *
     * Hint: The staff solution uses boot_map_region.
     *
     * LAB 5: Your code here:
     */

    if(ROUNDDOWN(pa, PGSIZE) != pa) {
        panic("Mmio physaddr not page-aligned");
    }

    size = ROUNDUP(size, PGSIZE);

    if(base + size > MMIOLIM) {
        panic("Tried to map mmio region larger than MMIOLIM");
    }

    boot_map_region(kern_pgdir, base, size, pa, PTE_BIT_RW | PTE_BIT_PRESENT | PTE_BIT_WRITETHROUGH | PTE_BIT_DISABLECACHE);
    base += size;
    return (void*)base - size;
}

static uintptr_t user_mem_check_addr;

/*
 * Check that an environment is allowed to access the range of memory
 * [va, va+len) with permissions 'perm | PTE_P'.
 * Normally 'perm' will contain PTE_U at least, but this is not required.
 * 'va' and 'len' need not be page-aligned; you must test every page that
 * contains any of that range.  You will test either 'len/PGSIZE',
 * 'len/PGSIZE + 1', or 'len/PGSIZE + 2' pages.
 *
 * A user program can access a virtual address if (1) the address is below
 * ULIM, and (2) the page table gives it permission.  These are exactly
 * the tests you should implement here.
 *
 * If there is an error, set the 'user_mem_check_addr' variable to the first
 * erroneous virtual address.
 *
 * Returns 0 if the user program can access this range of addresses,
 * and -E_FAULT otherwise.
 */
int user_mem_check(struct env *env, const void *va, size_t len, int perm)
{

    /* Make sure perm has user permissions as check */
    perm |= PTE_BIT_USER;

    /* Setup variables */
    int hasperm = 1; //preemtive yes
    int validaddr = 1; //preemtive yes

    /* setup rounddown to page address */
    uint32_t addr  = (uint32_t) va;
    user_mem_check_addr = addr;
    addr = ROUNDDOWN(addr, PGSIZE);

    //Check all pages
    for(uint32_t i = addr; i <= addr+len;) {
        /* Get pte entry pointer */
        pte_t * pentry = pgdir_walk(env->env_pgdir, (void*) i, 0);

        /* check pte permissions */
        hasperm &= perm == ((*pentry) & perm);

        /* Check address */
        validaddr &= i < ULIM;

        /* Return if problem found */
        if (!(hasperm && validaddr))
            return -E_FAULT;

        /* increment i */
        if ((*pentry) & PDE_BIT_HUGE)
            i += 1024*PGSIZE;
        else
            i += PGSIZE;

        /* Set current memory check */
        user_mem_check_addr = i;

    }


    return 0;

}
/*
 * Checks that environment 'env' is allowed to access the range
 * of memory [va, va+len) with permissions 'perm | PTE_U | PTE_P'.
 * If it can, then the function simply returns.
 * If it cannot, 'env' is destroyed and, if env is the current
 * environment, this function will not return.
 */
void user_mem_assert(struct env *env, const void *va, size_t len, int perm)
{
    if (user_mem_check(env, va, len, perm | PTE_U) < 0) {
        cprintf("[%08x] user_mem_check assertion failure for "
            "va %08x\n", env->env_id, user_mem_check_addr);
        env_destroy(env);   /* may not return */
    }
}


/***************************************************************
 * Checking functions.
 ***************************************************************/

/*
 * Check that the pages on the page_free_list are reasonable.
 */
static void check_page_free_list(bool only_low_memory) {
    struct page_info *pp;
    unsigned pdx_limit = only_low_memory ? 1 : NPDENTRIES;
    int nfree_basemem = 0, nfree_extmem = 0;
    char *first_free_page;

    if (!page_free_list)
        panic("'page_free_list' is a null pointer!");

    if (only_low_memory) {
        /* Move pages with lower addresses first in the free list, since
         * entry_pgdir does not map all pages. */
        struct page_info *pp1, *pp2;
        struct page_info **tp[2] = {&pp1, &pp2};
        for (pp = page_free_list; pp; pp = pp->pp_link) {
            int pagetype = PDX(page2pa(pp)) >= pdx_limit;
            *tp[pagetype] = pp;
            tp[pagetype] = &pp->pp_link;
        }
        *tp[1] = 0;
        *tp[0] = pp2;
        page_free_list = pp1;
    }

    /* if there's a page that shouldn't be on the free list,
     * try to make sure it eventually causes trouble. */
    for (pp = page_free_list; pp; pp = pp->pp_link)
        if (PDX(page2pa(pp)) < pdx_limit)
            memset(page2kva(pp), 0x97, 128);

    first_free_page = (char *) boot_alloc(0);
    for (pp = page_free_list; pp; pp = pp->pp_link) {
        /* check that we didn't corrupt the free list itself */
        assert(pp >= pages);
        assert(pp < pages + npages);
        assert(((char *) pp - (char *) pages) % sizeof (*pp) == 0);

        /* check a few pages that shouldn't be on the free list */
        assert(page2pa(pp) != 0);
        assert(page2pa(pp) != IOPHYSMEM);
        assert(page2pa(pp) != EXTPHYSMEM - PGSIZE);
        assert(page2pa(pp) != EXTPHYSMEM);
        assert(page2pa(pp) < EXTPHYSMEM || (char *) page2kva(pp) >= first_free_page);

        if (page2pa(pp) < EXTPHYSMEM)
            ++nfree_basemem;
        else
            ++nfree_extmem;
    }

    assert(nfree_basemem > 0);
    assert(nfree_extmem > 0);
}

/*
 * Check the physical page allocator (page_alloc(), page_free(),
 * and page_init()).
 */
static void check_page_alloc(void) {
    struct page_info *pp, *pp0, *pp1, *pp2;
    struct page_info *php0, *php1, *php2;
    int nfree, total_free;
    struct page_info *fl;
    char *c;
    int i;

    if (!pages)
        panic("'pages' is a null pointer!");

    /* check number of free pages */
    for (pp = page_free_list, nfree = 0; pp; pp = pp->pp_link)
        ++nfree;
    total_free = nfree;

    /* should be able to allocate three pages */
    pp0 = pp1 = pp2 = 0;
    assert((pp0 = page_alloc(0)));
    assert((pp1 = page_alloc(0)));
    assert((pp2 = page_alloc(0)));

    assert(pp0);
    assert(pp1 && pp1 != pp0);
    assert(pp2 && pp2 != pp1 && pp2 != pp0);
    assert(page2pa(pp0) < npages * PGSIZE);
    assert(page2pa(pp1) < npages * PGSIZE);
    assert(page2pa(pp2) < npages * PGSIZE);

    /* temporarily steal the rest of the free pages.
     *
     * Lab 1 Bonus:
     * For the bonus, if you go for a different design for the page allocator,
     * then do update here suitably to simulate a no-free-memory situation */
    fl = page_free_list;
    page_free_list = 0;

    /* should be no free memory */
    assert(!page_alloc(0));

    /* free and re-allocate? */
    page_free(pp0);
    page_free(pp1);
    page_free(pp2);
    pp0 = pp1 = pp2 = 0;
    assert((pp0 = page_alloc(0)));
    assert((pp1 = page_alloc(0)));
    assert((pp2 = page_alloc(0)));
    assert(pp0);
    assert(pp1 && pp1 != pp0);
    assert(pp2 && pp2 != pp1 && pp2 != pp0);
    assert(!page_alloc(0));

    /* test flags */
    memset(page2kva(pp0), 1, PGSIZE);
    page_free(pp0);
    assert((pp = page_alloc(ALLOC_ZERO)));
    assert(pp && pp0 == pp);
    c = page2kva(pp);
    for (i = 0; i < PGSIZE; i++)
        assert(c[i] == 0);

    /* give free list back */
    page_free_list = fl;

    /* free the pages we took */
    page_free(pp0);
    page_free(pp1);
    page_free(pp2);

    /* number of free pages should be the same */
    for (pp = page_free_list; pp; pp = pp->pp_link)
        --nfree;
    assert(nfree == 0);

    cprintf("[4K] check_page_alloc() succeeded!\n");

    /* test allocation of huge page */
    pp0 = pp1 = php0 = 0;
    assert((pp0 = page_alloc(0)));
    assert((php0 = page_alloc(ALLOC_HUGE)));
    assert((pp1 = page_alloc(0)));
    assert(pp0);
    assert(php0 && php0 != pp0);
    assert(pp1 && pp1 != php0 && pp1 != pp0);
    assert(0 == (page2pa(php0) % 1024 * PGSIZE));
    if (page2pa(pp1) > page2pa(php0)) {
        assert(page2pa(pp1) - page2pa(php0) >= 1024 * PGSIZE);
    }

    /* free and reallocate 2 huge pages */
    page_free(php0);
    page_free(pp0);
    page_free(pp1);
    php0 = php1 = pp0 = pp1 = 0;
    assert((php0 = page_alloc(ALLOC_HUGE)));
    assert((php1 = page_alloc(ALLOC_HUGE)));

    /* Is the inter-huge-page difference right? */
    if (page2pa(php1) > page2pa(php0)) {
        assert(page2pa(php1) - page2pa(php0) >= 1024 * PGSIZE);
    } else {
        assert(page2pa(php0) - page2pa(php1) >= 1024 * PGSIZE);
    }

    /* free the huge pages we took */
    page_free(php0);
    page_free(php1);

    /* number of free pages should be the same */
    nfree = total_free;
    for (pp = page_free_list; pp; pp = pp->pp_link)
        --nfree;
    assert(nfree == 0);

    cprintf("[4M] check_page_alloc() succeeded!\n");
}

/*
 * Checks that the kernel part of virtual address space
 * has been setup roughly correctly (by mem_init()).
 *
 * This function doesn't test every corner case,
 * but it is a pretty good sanity check.
 */
static void check_kern_pgdir(void) {
    uint32_t i, n;
    pde_t *pgdir;

    pgdir = kern_pgdir;

    /* check pages array */
    n = ROUNDUP(npages * sizeof (struct page_info), PGSIZE);
    for (i = 0; i < n; i += PGSIZE)
        assert(check_va2pa(pgdir, UPAGES + i) == PADDR(pages) + i);

    /* check envs array (new test for lab 3) */
    n = ROUNDUP(NENV*sizeof(struct env), PGSIZE);
    for (i = 0; i < n; i += PGSIZE)
        assert(check_va2pa(pgdir, UENVS + i) == PADDR(envs) + i);

    /* check phys mem */
    for (i = 0; i < npages * PGSIZE; i += PGSIZE)
        assert(check_va2pa(pgdir, KERNBASE + i) == i);

    /* check kernel stack */
    /* (updated in LAB 6 to check per-CPU kernel stacks) */
    for (n = 0; n < NCPU; n++) {
        uint32_t base = KSTACKTOP - (KSTKSIZE + KSTKGAP) * (n + 1);
        for (i = 0; i < KSTKSIZE; i += PGSIZE)
            assert(check_va2pa(pgdir, base + KSTKGAP + i)
                == PADDR(percpu_kstacks[n]) + i);
        for (i = 0; i < KSTKGAP; i += PGSIZE)
            assert(check_va2pa(pgdir, base + i) == ~0);
    }

    /* check PDE permissions */
    for (i = 0; i < NPDENTRIES; i++) {
        switch (i) {
        case PDX(UVPT):
        case PDX(KSTACKTOP - 1):
        case PDX(UPAGES):
        case PDX(UENVS):
        case PDX(MMIOBASE):
            assert(pgdir[i] & PTE_P);
            break;
        default:
            if (i >= PDX(KERNBASE)) {
                assert(pgdir[i] & PTE_P);
                assert(pgdir[i] & PTE_W);
            } else
                assert(pgdir[i] == 0);
            break;
        }
    }
    cprintf("check_kern_pgdir() succeeded!\n");
}

/*
 * This function returns the physical address of the page containing 'va',
 * defined by the page directory 'pgdir'.  The hardware normally performs
 * this functionality for us!  We define our own version to help check
 * the check_kern_pgdir() function; it shouldn't be used elsewhere.
 */
static physaddr_t check_va2pa(pde_t *pgdir, uintptr_t va) {
    pte_t *p;

    pgdir = &pgdir[PDX(va)];
    if (!(*pgdir & PTE_P))
        return ~0;
    p = (pte_t*) KADDR(PTE_ADDR(*pgdir));
    if (!(p[PTX(va)] & PTE_P))
        return ~0;
    return PTE_ADDR(p[PTX(va)]);
}

/* Check page_insert, page_remove, &c */
static void check_page(void) {
    struct page_info *pp, *pp0, *pp1, *pp2;
    struct page_info *fl;
    pte_t *ptep, *ptep1;
    void *va;
    uintptr_t mm1, mm2;
    int i;
    extern pde_t entry_pgdir[];

    /* should be able to allocate three pages */
    pp0 = pp1 = pp2 = 0;
    assert((pp0 = page_alloc(0)));
    assert((pp1 = page_alloc(0)));
    assert((pp2 = page_alloc(0)));

    assert(pp0);
    assert(pp1 && pp1 != pp0);
    assert(pp2 && pp2 != pp1 && pp2 != pp0);

    /* temporarily steal the rest of the free pages.
     *
     * Lab 1 Bonus:
     * For the bonus, if you had chosen a different design for
     * the page allocator, then do update here suitably to
     * simulate a no-free-memory situation */
    fl = page_free_list;
    page_free_list = 0;

    /* should be no free memory */
    assert(!page_alloc(0));

    /* there is no page allocated at address 0 */
    assert(page_lookup(kern_pgdir, (void *) 0x0, &ptep) == NULL);

    /* there is no free memory, so we can't allocate a page table */
    assert(page_insert(kern_pgdir, pp1, 0x0, PTE_W) < 0);

    /* free pp0 and try again: pp0 should be used for page table */
    page_free(pp0);
    assert(page_insert(kern_pgdir, pp1, 0x0, PTE_W) == 0);
    assert(PTE_ADDR(kern_pgdir[0]) == page2pa(pp0));
    assert(check_va2pa(kern_pgdir, 0x0) == page2pa(pp1));
    assert(pp1->pp_ref == 1);
    assert(pp0->pp_ref == 1);

    /* should be able to map pp2 at PGSIZE because pp0 is already allocated for page table */
    assert(page_insert(kern_pgdir, pp2, (void*) PGSIZE, PTE_W) == 0);
    assert(check_va2pa(kern_pgdir, PGSIZE) == page2pa(pp2));
    assert(pp2->pp_ref == 1);

    /* should be no free memory */
    assert(!page_alloc(0));

    /* should be able to map pp2 at PGSIZE because it's already there */
    assert(page_insert(kern_pgdir, pp2, (void*) PGSIZE, PTE_W) == 0);
    assert(check_va2pa(kern_pgdir, PGSIZE) == page2pa(pp2));
    assert(pp2->pp_ref == 1);

    /* pp2 should NOT be on the free list
     * could happen in ref counts are handled sloppily in page_insert */
    assert(!page_alloc(0));

    /* check that pgdir_walk returns a pointer to the pte */
    ptep = (pte_t *) KADDR(PTE_ADDR(kern_pgdir[PDX(PGSIZE)]));
    assert(pgdir_walk(kern_pgdir, (void*) PGSIZE, 0) == ptep + PTX(PGSIZE));

    /* should be able to change permissions too. */
    assert(page_insert(kern_pgdir, pp2, (void*) PGSIZE, PTE_W | PTE_U) == 0);
    assert(check_va2pa(kern_pgdir, PGSIZE) == page2pa(pp2));
    assert(pp2->pp_ref == 1);
    assert(*pgdir_walk(kern_pgdir, (void*) PGSIZE, 0) & PTE_U);
    assert(kern_pgdir[0] & PTE_U);

    /* should be able to remap with fewer permissions */
    assert(page_insert(kern_pgdir, pp2, (void*) PGSIZE, PTE_W) == 0);
    assert(*pgdir_walk(kern_pgdir, (void*) PGSIZE, 0) & PTE_W);
    assert(!(*pgdir_walk(kern_pgdir, (void*) PGSIZE, 0) & PTE_U));

    /* Should not be able to map at PTSIZE because need free page for page
     * table. */

    assert(page_insert(kern_pgdir, pp0, (void*) PTSIZE, PTE_W) < 0);

    /* insert pp1 at PGSIZE (replacing pp2) */
    assert(page_insert(kern_pgdir, pp1, (void*) PGSIZE, PTE_W) == 0);
    assert(!(*pgdir_walk(kern_pgdir, (void*) PGSIZE, 0) & PTE_U));

    /* should have pp1 at both 0 and PGSIZE, pp2 nowhere, ... */
    assert(check_va2pa(kern_pgdir, 0) == page2pa(pp1));
    assert(check_va2pa(kern_pgdir, PGSIZE) == page2pa(pp1));
    /* ... and ref counts should reflect this */
    assert(pp1->pp_ref == 2);
    assert(pp2->pp_ref == 0);

    /* pp2 should be returned by page_alloc */
    assert((pp = page_alloc(0)) && pp == pp2);

    /* unmapping pp1 at 0 should keep pp1 at PGSIZE */
    page_remove(kern_pgdir, 0x0);
    assert(check_va2pa(kern_pgdir, 0x0) == ~0);
    assert(check_va2pa(kern_pgdir, PGSIZE) == page2pa(pp1));
    assert(pp1->pp_ref == 1);
    assert(pp2->pp_ref == 0);

    /* test re-inserting pp1 at PGSIZE */
    assert(page_insert(kern_pgdir, pp1, (void*) PGSIZE, 0) == 0);
    assert(pp1->pp_ref);
    assert(pp1->pp_link == NULL);

    /* unmapping pp1 at PGSIZE should free it */
    page_remove(kern_pgdir, (void*) PGSIZE);
    assert(check_va2pa(kern_pgdir, 0x0) == ~0);
    assert(check_va2pa(kern_pgdir, PGSIZE) == ~0);
    assert(pp1->pp_ref == 0);
    assert(pp2->pp_ref == 0);

    /* so it should be returned by page_alloc */
    assert((pp = page_alloc(0)) && pp == pp1);

    /* should be no free memory */
    assert(!page_alloc(0));

    /* forcibly take pp0 back */
    assert(PTE_ADDR(kern_pgdir[0]) == page2pa(pp0));
    kern_pgdir[0] = 0;
    assert(pp0->pp_ref == 1);
    pp0->pp_ref = 0;

    /* check pointer arithmetic in pgdir_walk */
    page_free(pp0);
    va = (void*) (PGSIZE * NPDENTRIES + PGSIZE);
    ptep = pgdir_walk(kern_pgdir, va, 1);
    ptep1 = (pte_t *) KADDR(PTE_ADDR(kern_pgdir[PDX(va)]));
    assert(ptep == ptep1 + PTX(va));
    kern_pgdir[PDX(va)] = 0;
    pp0->pp_ref = 0;

    /* check that new page tables get cleared */
    memset(page2kva(pp0), 0xFF, PGSIZE);
    page_free(pp0);
    pgdir_walk(kern_pgdir, 0x0, 1);
    ptep = (pte_t *) page2kva(pp0);
    for (i = 0; i < NPTENTRIES; i++)
        assert((ptep[i] & PTE_P) == 0);
    kern_pgdir[0] = 0;
    pp0->pp_ref = 0;

    /* give free list back */
    page_free_list = fl;

    /* free the pages we took */
    page_free(pp0);
    page_free(pp1);
    page_free(pp2);

    /* test mmio_map_region */
    mm1 = (uintptr_t) mmio_map_region(0, 4097);
    mm2 = (uintptr_t) mmio_map_region(0, 4096);
    /* check that they're in the right region */
    assert(mm1 >= MMIOBASE && mm1 + 8096 < MMIOLIM);
    assert(mm2 >= MMIOBASE && mm2 + 8096 < MMIOLIM);
    /* check that they're page-aligned */
    assert(mm1 % PGSIZE == 0 && mm2 % PGSIZE == 0);
    /* check that they don't overlap */
    assert(mm1 + 8096 <= mm2);
    /* check page mappings */
    assert(check_va2pa(kern_pgdir, mm1) == 0);
    assert(check_va2pa(kern_pgdir, mm1+PGSIZE) == PGSIZE);
    assert(check_va2pa(kern_pgdir, mm2) == 0);
    assert(check_va2pa(kern_pgdir, mm2+PGSIZE) == ~0);
    /* check permissions */
    assert(*pgdir_walk(kern_pgdir, (void*) mm1, 0) & (PTE_W|PTE_PWT|PTE_PCD));
    assert(!(*pgdir_walk(kern_pgdir, (void*) mm1, 0) & PTE_U));
    /* clear the mappings */
    *pgdir_walk(kern_pgdir, (void*) mm1, 0) = 0;
    *pgdir_walk(kern_pgdir, (void*) mm1 + PGSIZE, 0) = 0;
    *pgdir_walk(kern_pgdir, (void*) mm2, 0) = 0;

    cprintf("check_page() succeeded!\n");
}

/* Check page_insert, page_remove, &c, with an installed kern_pgdir */
static void check_page_installed_pgdir(void) {
    struct page_info *pp, *pp0, *pp1, *pp2;
    struct page_info *fl;
    pte_t *ptep, *ptep1;
    uintptr_t va;
    int i;

    /* check that we can read and write installed pages */
    pp1 = pp2 = 0;
    assert((pp0 = page_alloc(0)));
    assert((pp1 = page_alloc(0)));
    assert((pp2 = page_alloc(0)));
    page_free(pp0);
    memset(page2kva(pp1), 1, PGSIZE);
    memset(page2kva(pp2), 2, PGSIZE);
    page_insert(kern_pgdir, pp1, (void*) PGSIZE, PTE_W);
    assert(pp1->pp_ref == 1);
    assert(*(uint32_t *) PGSIZE == 0x01010101U);
    page_insert(kern_pgdir, pp2, (void*) PGSIZE, PTE_W);
    assert(*(uint32_t *) PGSIZE == 0x02020202U);
    assert(pp2->pp_ref == 1);
    assert(pp1->pp_ref == 0);
    *(uint32_t *) PGSIZE = 0x03030303U;
    assert(*(uint32_t *) page2kva(pp2) == 0x03030303U);
    page_remove(kern_pgdir, (void*) PGSIZE);
    assert(pp2->pp_ref == 0);

    /* forcibly take pp0 back */
    assert(PTE_ADDR(kern_pgdir[0]) == page2pa(pp0));
    kern_pgdir[0] = 0;
    assert(pp0->pp_ref == 1);
    pp0->pp_ref = 0;

    /* free the pages we took */
    page_free(pp0);

    cprintf("check_page_installed_pgdir() succeeded!\n");
}

/* Check pgdir_walk() for huge page support */
static void check_page_hugepages(void) {
    struct page_info *php0;
    assert(php0 = page_alloc(ALLOC_HUGE));
    assert(page_insert(kern_pgdir, php0, (void *) (1024 * PGSIZE), PTE_W | PTE_PS) == 0);
    assert(php0->pp_ref == 1);
    memset(page2kva(php0), 1, PGSIZE);
    assert(*(uint32_t *) (1024 * PGSIZE) == 0x01010101U);

    /* Access the second 4K-page within the huge page */
    memset(page2kva(php0 + 1), 2, PGSIZE);
    assert(*(uint32_t *) (1025 * PGSIZE) == 0x02020202U);

    /* Writing to the last 4K-page within the huge page works? */
    *(uint32_t *) (2 * 1024 * PGSIZE - 42) = 0x42424242U;
    assert(*(uint32_t *) (2 * 1024 * PGSIZE - 42) == 0x42424242U);

    /* Are the underlying frames consecutive? */
    memset(page2kva(php0 + 1021), 0x37, PGSIZE);
    memset(page2kva(php0 + 1022), 0x38, PGSIZE);
    assert(*(uint32_t *) ((1024 + 1021) * PGSIZE) == 0x37373737U);
    assert(*(uint32_t *) ((1024 + 1022) * PGSIZE) == 0x38383838U);

    /* Check pgdir_walk for the page and the PSE bit */
    pte_t *p_pte1, *p_pte2;
    p_pte1 = pgdir_walk(kern_pgdir, (void*) (1024 * PGSIZE), 0);
    assert(NULL != p_pte1);
    assert(*p_pte1 & PTE_PS);
    p_pte2 = pgdir_walk(kern_pgdir, (void*) (1025 * PGSIZE), 0);
    assert(NULL != p_pte2);
    assert(p_pte1 == p_pte2);

    /* check page_remove() on the huge page */
    page_remove(kern_pgdir, (void*) (1024 * PGSIZE));
    assert(php0->pp_ref == 0);
    assert((php0 + 1022)->pp_ref == 0);

    /* check CREATE_HUGE flag */
    p_pte1 = pgdir_walk(kern_pgdir, (void*) (1024 * PGSIZE), CREATE_HUGE);
    assert(NULL != p_pte1);
    assert(php0 = page_alloc(ALLOC_HUGE));
    assert(page_insert(kern_pgdir, php0, (void *) (2 * 1024 * PGSIZE), PTE_W | PTE_PS) == 0);
    page_remove(kern_pgdir, (void*) (2 * 1024 * PGSIZE));
    assert(php0->pp_ref == 0);

    cprintf("check_page_hugepages() succeeded!\n");
}
